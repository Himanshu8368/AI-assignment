{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boilerplate for AI Assignment â€” Knowledge Representation, Reasoning and Planning\n",
    "# CSE 643\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import networkx as nx\n",
    "from pyDatalog import pyDatalog\n",
    "from collections import defaultdict, deque\n",
    "# -------------------------------------we add this \n",
    "import pickle\n",
    "import os\n",
    "#-------------------------------------------end\n",
    "\n",
    "## ****IMPORTANT****\n",
    "## Don't import or use any other libraries other than defined above\n",
    "## Otherwise your code file will be rejected in the automated testing\n",
    "\n",
    "# ------------------ Global Variables ------------------\n",
    "route_to_stops = defaultdict(list)  # Mapping of route IDs to lists of stops\n",
    "trip_to_route = {}                   # Mapping of trip IDs to route IDs\n",
    "stop_trip_count = defaultdict(int)    # Count of trips for each stop\n",
    "fare_rules = {}                      # Mapping of route IDs to fare information\n",
    "merged_fare_df = None                # To be initialized in create_kb()\n",
    "\n",
    "# Load static data from GTFS (General Transit Feed Specification) files\n",
    "df_stops = pd.read_csv('GTFS/stops.txt')\n",
    "df_routes = pd.read_csv('GTFS/routes.txt')\n",
    "df_stop_times = pd.read_csv('GTFS/stop_times.txt')\n",
    "df_fare_attributes = pd.read_csv('GTFS/fare_attributes.txt')\n",
    "df_trips = pd.read_csv('GTFS/trips.txt')\n",
    "df_fare_rules = pd.read_csv('GTFS/fare_rules.txt')\n",
    "\n",
    "# ------------------ Function Definitions ------------------\n",
    "\n",
    "# ------------------ Function Implementations ------------------\n",
    "\n",
    "# def create_kb():\n",
    "#     \"\"\"\n",
    "#     Create the knowledge base by populating global variables with information from loaded datasets.\n",
    "#     This function establishes the relationships between routes, trips, stops, and fare rules.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     global route_to_stops, trip_to_route, stop_trip_count, fare_rules, merged_fare_df\n",
    "\n",
    "#     # Step 1: Create trip_id to route_id mapping using df_trips.\n",
    "#     for _, row in df_trips.iterrows():\n",
    "#         trip_to_route[row['trip_id']] = row['route_id']\n",
    "\n",
    "#     # Step 2: Map route_id to a list of stops in the correct sequence using df_stop_times.\n",
    "#     for _, row in df_stop_times.iterrows():\n",
    "#         route_id = trip_to_route.get(row['trip_id'])\n",
    "#         if route_id:\n",
    "#             # Ensure we store stops with stop_sequence information\n",
    "#             if route_id not in route_to_stops:\n",
    "#                 route_to_stops[route_id] = []\n",
    "#             # Add a tuple with (stop_id, stop_sequence)\n",
    "#             route_to_stops[route_id].append((row['stop_id'], row['stop_sequence']))\n",
    "\n",
    "#     # Step 3: Ensure each route only has unique stops by removing duplicates and sorting by stop_sequence.\n",
    "#     for route_id, stops in route_to_stops.items():\n",
    "#         # Filter out any entries that aren't tuples, to avoid errors\n",
    "#         stops = [stop for stop in stops if isinstance(stop, tuple) and len(stop) == 2]\n",
    "        \n",
    "#         # Now, remove duplicates and sort by stop_sequence\n",
    "#         unique_stops = sorted(set(stops), key=lambda x: x[1])  # Sort by stop sequence\n",
    "#         route_to_stops[route_id] = [stop_id for stop_id, _ in unique_stops]\n",
    "\n",
    "#     # Step 4: Count trips for each stop using df_stop_times.\n",
    "#     for _, row in df_stop_times.iterrows():\n",
    "#         stop_trip_count[row['stop_id']] += 1\n",
    "\n",
    "#     # Step 5: Create fare rules for routes using df_fare_rules and df_fare_attributes.\n",
    "#     fare_rules = df_fare_rules.set_index('route_id').to_dict()['fare_id']\n",
    "#     fare_attributes = df_fare_attributes.set_index('fare_id').to_dict()\n",
    "\n",
    "#     # Step 6: Merge fare_rules and fare_attributes into a single DataFrame for easy access.\n",
    "#     merged_fare_df = df_fare_rules.merge(df_fare_attributes, on='fare_id')\n",
    "\n",
    "def create_kb():\n",
    "    \"\"\"\n",
    "    Create or load the knowledge base by populating global variables with information from loaded datasets.\n",
    "    This function establishes the relationships between routes, trips, stops, and fare rules.\n",
    "    If a knowledge base file already exists, it loads the data from the file instead of reprocessing.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    global route_to_stops, trip_to_route, stop_trip_count, fare_rules, merged_fare_df\n",
    "\n",
    "    # Define the file path for the .pkl file\n",
    "    kb_file_path = 'knowledge_base.pkl'\n",
    "\n",
    "    # Check if the knowledge base file already exists\n",
    "    if os.path.exists(kb_file_path):\n",
    "        # Load data from the .pkl file\n",
    "        with open(kb_file_path, 'rb') as f:\n",
    "            route_to_stops, trip_to_route, stop_trip_count, fare_rules, merged_fare_df = pickle.load(f)\n",
    "        print(\"Knowledge base loaded from file.\")\n",
    "        return  # Exit function after loading\n",
    "\n",
    "    # Step 1: Create trip_id to route_id mapping using df_trips.\n",
    "    for _, row in df_trips.iterrows():\n",
    "        trip_to_route[row['trip_id']] = row['route_id']\n",
    "\n",
    "    # Step 2: Map route_id to a list of stops in the correct sequence using df_stop_times.\n",
    "    for _, row in df_stop_times.iterrows():\n",
    "        route_id = trip_to_route.get(row['trip_id'])\n",
    "        if route_id:\n",
    "            # Ensure we store stops with stop_sequence information\n",
    "            if route_id not in route_to_stops:\n",
    "                route_to_stops[route_id] = []\n",
    "            # Add a tuple with (stop_id, stop_sequence)\n",
    "            route_to_stops[route_id].append((row['stop_id'], row['stop_sequence']))\n",
    "\n",
    "    # Step 3: Ensure each route only has unique stops by removing duplicates and sorting by stop_sequence.\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        stops = [stop for stop in stops if isinstance(stop, tuple) and len(stop) == 2]\n",
    "        unique_stops = sorted(set(stops), key=lambda x: x[1])  # Sort by stop sequence\n",
    "        route_to_stops[route_id] = [stop_id for stop_id, _ in unique_stops]\n",
    "\n",
    "    # Step 4: Count trips for each stop using df_stop_times.\n",
    "    for _, row in df_stop_times.iterrows():\n",
    "        stop_trip_count[row['stop_id']] += 1\n",
    "\n",
    "    # Step 5: Create fare rules for routes using df_fare_rules and df_fare_attributes.\n",
    "    fare_rules = df_fare_rules.set_index('route_id').to_dict()['fare_id']\n",
    "    fare_attributes = df_fare_attributes.set_index('fare_id').to_dict()\n",
    "\n",
    "    # Step 6: Merge fare_rules and fare_attributes into a single DataFrame for easy access.\n",
    "    merged_fare_df = df_fare_rules.merge(df_fare_attributes, on='fare_id')\n",
    "\n",
    "    # Save the populated knowledge base to a .pkl file for future use\n",
    "    with open(kb_file_path, 'wb') as f:\n",
    "        pickle.dump((route_to_stops, trip_to_route, stop_trip_count, fare_rules, merged_fare_df), f)\n",
    "    print(\"Knowledge base created and saved to file.\")\n",
    "\n",
    "def get_busiest_routes():\n",
    "    \"\"\"\n",
    "    Identify the top 5 busiest routes based on the number of trips.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "              - route_id (int): The ID of the route.\n",
    "              - trip_count (int): The number of trips for that route.\n",
    "    \"\"\"\n",
    "    # Count the number of trips for each route using trip_to_route.\n",
    "    route_trip_count = defaultdict(int)\n",
    "    for trip_id, route_id in trip_to_route.items():\n",
    "        route_trip_count[route_id] += 1\n",
    "\n",
    "    # Sort routes by trip count in descending order and get the top 5.\n",
    "    top_routes = sorted(route_trip_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_routes\n",
    "\n",
    "\n",
    "def get_most_frequent_stops():\n",
    "    \"\"\"\n",
    "    Identify the top 5 stops with the highest number of trips.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "              - stop_id (int): The ID of the stop.\n",
    "              - trip_count (int): The number of trips for that stop.\n",
    "    \"\"\"\n",
    "    # Sort stops by trip count in descending order and get the top 5.\n",
    "    top_stops = sorted(stop_trip_count.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_stops\n",
    "\n",
    "\n",
    "def get_top_5_busiest_stops():\n",
    "    \"\"\"\n",
    "    Identify the top 5 stops with the highest number of different routes passing through them.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "              - stop_id (int): The ID of the stop.\n",
    "              - route_count (int): The number of routes passing through that stop.\n",
    "    \"\"\"\n",
    "    # Create a mapping from stop_id to unique route_ids passing through it.\n",
    "    stop_route_count = defaultdict(set)\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        for stop_id in stops:\n",
    "            stop_route_count[stop_id].add(route_id)\n",
    "\n",
    "    # Count unique routes per stop and get the top 5 busiest stops.\n",
    "    stop_route_counts = {stop_id: len(routes) for stop_id, routes in stop_route_count.items()}\n",
    "    top_stops = sorted(stop_route_counts.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "    return top_stops\n",
    "\n",
    "\n",
    "def get_stops_with_one_direct_route():\n",
    "    \"\"\"\n",
    "    Identify the top 5 pairs of consecutive stops (start and end) connected by exactly one direct route.\n",
    "    The pairs are sorted by the combined frequency of trips passing through both stops.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "              - pair (tuple): A tuple with two stop IDs (stop_1, stop_2).\n",
    "              - route_id (int): The ID of the route connecting the two stops.\n",
    "    \"\"\"\n",
    "    # Map pairs of consecutive stops to routes.\n",
    "    stop_pairs = defaultdict(list)\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        for i in range(len(stops) - 1):\n",
    "            pair = (stops[i], stops[i + 1])\n",
    "            stop_pairs[pair].append(route_id)\n",
    "\n",
    "    # Filter pairs that only have one direct route and sort by trip frequency.\n",
    "    one_route_pairs = [(pair, routes[0]) for pair, routes in stop_pairs.items() if len(routes) == 1]\n",
    "    one_route_pairs_sorted = sorted(one_route_pairs, \n",
    "                                    key=lambda x: stop_trip_count[x[0][0]] + stop_trip_count[x[0][1]], \n",
    "                                    reverse=True)[:5]\n",
    "    return one_route_pairs_sorted\n",
    "\n",
    "\n",
    "# Function to get merged fare DataFrame\n",
    "# No need to change this function\n",
    "def get_merged_fare_df():\n",
    "    \"\"\"\n",
    "    Retrieve the merged fare DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: The merged fare DataFrame containing fare rules and attributes.\n",
    "    \"\"\"\n",
    "    global merged_fare_df\n",
    "    return merged_fare_df\n",
    "\n",
    "\n",
    "# Visualize the stop-route graph interactively\n",
    "def visualize_stop_route_graph_interactive(route_to_stops):\n",
    "    \"\"\"\n",
    "    Visualize the stop-route graph using Plotly for interactive exploration.\n",
    "\n",
    "    Args:\n",
    "        route_to_stops (dict): A dictionary mapping route IDs to lists of stops.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # Create a NetworkX graph\n",
    "    G = nx.Graph()\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        for i in range(len(stops) - 1):\n",
    "            G.add_edge(stops[i], stops[i + 1], route=route_id)\n",
    "\n",
    "    # Plot with Plotly for interactivity\n",
    "    pos = nx.spring_layout(G)\n",
    "    edge_x = []\n",
    "    edge_y = []\n",
    "    for edge in G.edges():\n",
    "        x0, y0 = pos[edge[0]]\n",
    "        x1, y1 = pos[edge[1]]\n",
    "        edge_x += [x0, x1, None]\n",
    "        edge_y += [y0, y1, None]\n",
    "\n",
    "    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')\n",
    "    node_x = []\n",
    "    node_y = []\n",
    "    for node in G.nodes():\n",
    "        x, y = pos[node]\n",
    "        node_x.append(x)\n",
    "        node_y.append(y)\n",
    "\n",
    "    node_trace = go.Scatter(x=node_x, y=node_y, mode='markers', marker=dict(size=10, color='blue'))\n",
    "    fig = go.Figure(data=[edge_trace, node_trace])\n",
    "    fig.update_layout(showlegend=False)\n",
    "    fig.show()\n",
    "\n",
    "# import plotly.graph_objs as go\n",
    "# import random\n",
    "\n",
    "# # Function to visualize the stop-route graph interactively\n",
    "# def visualize_stop_route_graph_interactive(route_to_stops):\n",
    "#     \"\"\"\n",
    "#     Visualize the stop-route graph using Plotly for interactive exploration.\n",
    "\n",
    "#     Args:\n",
    "#         route_to_stops (dict): A dictionary mapping route IDs to lists of stops.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     # Generate random 3D coordinates for each stop\n",
    "#     stop_coordinates = {}\n",
    "#     for route_id, stops in route_to_stops.items():\n",
    "#         for stop in stops:\n",
    "#             if stop not in stop_coordinates:\n",
    "#                 stop_coordinates[stop] = (random.uniform(0, 100), random.uniform(0, 100), random.uniform(0, 100))\n",
    "\n",
    "#     # Create lists for edges (lines) between stops on the same route\n",
    "#     edge_x = []\n",
    "#     edge_y = []\n",
    "#     edge_z = []\n",
    "#     node_x = []\n",
    "#     node_y = []\n",
    "#     node_z = []\n",
    "#     node_text = []\n",
    "    \n",
    "#     for route_id, stops in route_to_stops.items():\n",
    "#         for i in range(len(stops) - 1):\n",
    "#             x0, y0, z0 = stop_coordinates[stops[i]]\n",
    "#             x1, y1, z1 = stop_coordinates[stops[i + 1]]\n",
    "#             edge_x.extend([x0, x1, None])\n",
    "#             edge_y.extend([y0, y1, None])\n",
    "#             edge_z.extend([z0, z1, None])\n",
    "        \n",
    "#         # Add each stop as a node\n",
    "#         for stop in stops:\n",
    "#             x, y, z = stop_coordinates[stop]\n",
    "#             node_x.append(x)\n",
    "#             node_y.append(y)\n",
    "#             node_z.append(z)\n",
    "#             node_text.append(f\"Stop: {stop}, Route: {route_id}\")\n",
    "\n",
    "#     # Create 3D scatter plot for nodes (stops)\n",
    "#     node_trace = go.Scatter3d(\n",
    "#         x=node_x, y=node_y, z=node_z,\n",
    "#         mode='markers',\n",
    "#         marker=dict(size=6, color='blue', opacity=0.8),\n",
    "#         text=node_text,\n",
    "#         hoverinfo='text'\n",
    "#     )\n",
    "\n",
    "#     # Create 3D line plot for edges (connections between stops)\n",
    "#     edge_trace = go.Scatter3d(\n",
    "#         x=edge_x, y=edge_y, z=edge_z,\n",
    "#         mode='lines',\n",
    "#         line=dict(color='black', width=2),\n",
    "#         hoverinfo='none'\n",
    "#     )\n",
    "\n",
    "#     # Create figure and add traces\n",
    "#     fig = go.Figure(data=[edge_trace, node_trace])\n",
    "\n",
    "#     # Update layout for better 3D visualization\n",
    "#     fig.update_layout(\n",
    "#         title=\"Stop-Route Graph Visualization\",\n",
    "#         scene=dict(\n",
    "#             xaxis=dict(title=\"X\"),\n",
    "#             yaxis=dict(title=\"Y\"),\n",
    "#             zaxis=dict(title=\"Z\")\n",
    "#         ),\n",
    "#         margin=dict(l=0, r=0, b=0, t=40)\n",
    "#     )\n",
    "\n",
    "#     # Show the interactive 3D graph\n",
    "#     fig.show()\n",
    "\n",
    "# Brute-Force Approach for finding direct routes\n",
    "def direct_route_brute_force(start_stop, end_stop):\n",
    "    \"\"\"\n",
    "    Find all valid routes between two stops using a brute-force method.\n",
    "\n",
    "    Args:\n",
    "        start_stop (int): The ID of the starting stop.\n",
    "        end_stop (int): The ID of the ending stop.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of route IDs (int) that connect the two stops directly.\n",
    "    \"\"\"\n",
    "    direct_routes = []\n",
    "\n",
    "    # Iterate through each route and its stops\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        if start_stop in stops and end_stop in stops:\n",
    "            # Check if end_stop appears after start_stop in the route\n",
    "            start_index = stops.index(start_stop)\n",
    "            end_index = stops.index(end_stop)\n",
    "            if start_index < end_index:\n",
    "                direct_routes.append(route_id)\n",
    "\n",
    "    return direct_routes\n",
    "\n",
    "# Initialize Datalog predicates for reasoning\n",
    "pyDatalog.create_terms('RouteHasStop, DirectRoute, OptimalRoute, X, Y, Z, R, R1, R2')  \n",
    "\n",
    "\n",
    "# def initialize_datalog():\n",
    "#     \"\"\"\n",
    "#     Initialize Datalog terms and predicates for reasoning about routes and stops.\n",
    "\n",
    "#     Returns:\n",
    "#         None\n",
    "#     \"\"\"\n",
    "#     pyDatalog.clear()  # Clear previous terms\n",
    "#     print(\"Terms initialized: DirectRoute, RouteHasStop, OptimalRoute\")  # Confirmation print\n",
    "\n",
    "#     # Define base predicates for routes and stops\n",
    "#     # RouteHasStop(R, X) means route R includes stop X\n",
    "#     # DirectRoute(X, Y, R) means there is a direct route R between stops X and Y\n",
    "#     DirectRoute(X, Y, R) <= RouteHasStop(R, X) & RouteHasStop(R, Y)\n",
    "    \n",
    "#     # Populate the knowledge base with data\n",
    "#     create_kb()  # This will populate the global route_to_stops data structure\n",
    "#     add_route_data(route_to_stops)  # Add route data to Datalog\n",
    "\n",
    "def initialize_datalog():\n",
    "    \"\"\"\n",
    "    Initialize Datalog terms and predicates for reasoning about routes and stops.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    pyDatalog.clear()  # Clear previous terms\n",
    "    # print(\"Terms initialized: DirectRoute, RouteHasStop, OptimalRoute\")  # Confirmation print\n",
    "\n",
    "    # Define base predicates for routes and stops\n",
    "    DirectRoute(X, Y, R) <= RouteHasStop(R, X) & RouteHasStop(R, Y)\n",
    "\n",
    "    # Populate the knowledge base with data\n",
    "    create_kb()  # This will populate the global route_to_stops data structure\n",
    "    add_route_data(route_to_stops)  # Add route data to Datalog\n",
    "\n",
    "\n",
    "\n",
    "def add_route_data(route_to_stops):\n",
    "    \"\"\"\n",
    "    Add the route data to Datalog for reasoning.\n",
    "\n",
    "    Args:\n",
    "        route_to_stops (dict): A dictionary mapping route IDs to lists of stops.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    for route_id, stops in route_to_stops.items():\n",
    "        for stop in stops:\n",
    "            # Add facts to the knowledge base\n",
    "            +RouteHasStop(route_id, stop)\n",
    "\n",
    "def query_direct_routes(start, end):\n",
    "    \"\"\"\n",
    "    Query for direct routes between two stops.\n",
    "\n",
    "    Args:\n",
    "        start (int): The ID of the starting stop.\n",
    "        end (int): The ID of the ending stop.\n",
    "\n",
    "    Returns:\n",
    "        list: A sorted list of route IDs (int) connecting the two stops directly.\n",
    "    \"\"\"\n",
    "    # Query for routes that have both start and end stops\n",
    "    query_result = DirectRoute(start, end, R)\n",
    "    \n",
    "    # Extract route IDs from the query result\n",
    "    # route_ids = (set(route_id for route_id, in query_result))\n",
    "    route_ids = sorted(set(route_id for route_id, in query_result))\n",
    "    return route_ids\n",
    "\n",
    "def forward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    \"\"\"\n",
    "    Perform forward chaining to find optimal routes considering transfers.\n",
    "\n",
    "    Args:\n",
    "        start_stop_id (int): The starting stop ID.\n",
    "        end_stop_id (int): The ending stop ID.\n",
    "        stop_id_to_include (int): The stop ID where a transfer occurs.\n",
    "        max_transfers (int): The maximum number of transfers allowed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique paths (list of tuples) that satisfy the criteria.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    # Step 1: Find all routes that start from the starting stop\n",
    "    direct_routes = query_direct_routes(start_stop_id, stop_id_to_include)\n",
    "    \n",
    "    # Step 2: For each direct route, find paths to the ending stop\n",
    "    for route_id in direct_routes:\n",
    "        # Get all stops on this route\n",
    "        stops_on_route = route_to_stops[route_id]\n",
    "        \n",
    "        # Check if via stop is included and find routes from there to the end stop\n",
    "        if stop_id_to_include in stops_on_route:\n",
    "            index_of_via = stops_on_route.index(stop_id_to_include)\n",
    "\n",
    "            # Check for paths from the via stop to the end stop within the same route\n",
    "            for stop_id in stops_on_route[index_of_via + 1:]:\n",
    "                if stop_id == end_stop_id:\n",
    "                    paths.append([(route_id, stop_id)])\n",
    "\n",
    "            # If transfers are allowed, find other routes from the via stop to the end stop\n",
    "            if max_transfers > 0:\n",
    "                transfer_routes = query_direct_routes(stop_id_to_include, end_stop_id)\n",
    "                for transfer_route_id in transfer_routes:\n",
    "                    # paths.append([(route_id, stop_id_to_include), (transfer_route_id, end_stop_id)])\n",
    "                    paths.append((route_id, stop_id_to_include,transfer_route_id))\n",
    "\n",
    "    return paths\n",
    "    # return direct_routes\n",
    "\n",
    "\n",
    "\n",
    "def backward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    \"\"\"\n",
    "    Perform backward chaining to find optimal routes considering transfers.\n",
    "\n",
    "    Args:\n",
    "        start_stop_id (int): The starting stop ID.\n",
    "        end_stop_id (int): The ending stop ID.\n",
    "        stop_id_to_include (int): The stop ID where a transfer occurs.\n",
    "        max_transfers (int): The maximum number of transfers allowed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique paths (list of tuples) that satisfy the criteria.\n",
    "    \"\"\"\n",
    "    paths = []\n",
    "\n",
    "    # Step 1: Find all routes that end at the ending stop\n",
    "    direct_routes = query_direct_routes(stop_id_to_include, end_stop_id)\n",
    "\n",
    "    # Step 2: For each direct route, find paths back to the starting stop\n",
    "    for route_id in direct_routes:\n",
    "        # Get all stops on this route\n",
    "        stops_on_route = route_to_stops[route_id]\n",
    "        \n",
    "        # Check if via stop is included and find routes from the start stop\n",
    "        if stop_id_to_include in stops_on_route:\n",
    "            index_of_via = stops_on_route.index(stop_id_to_include)\n",
    "\n",
    "            # Check for paths from the via stop to the start stop\n",
    "            for stop_id in reversed(stops_on_route[:index_of_via]):\n",
    "                if stop_id == start_stop_id:\n",
    "                    paths.append([(route_id, stop_id)])\n",
    "\n",
    "            # If transfers are allowed, find routes from the start stop to the via stop\n",
    "            if max_transfers > 0:\n",
    "                transfer_routes = query_direct_routes(start_stop_id, stop_id_to_include)\n",
    "                for transfer_route_id in transfer_routes:\n",
    "                    # paths.append([(transfer_route_id, start_stop_id), (route_id, stop_id_to_include)])\n",
    "                    paths.append((route_id, stop_id_to_include,transfer_route_id))\n",
    "\n",
    "    return paths\n",
    "    # return direct_routes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def pddl_planning(start_stop_id, end_stop_id, stop_id_to_include=None, max_transfers=3):\n",
    "#     \"\"\"\n",
    "#     Optimized PDDL-style planning to find routes with optional transfers.\n",
    "\n",
    "#     Args:\n",
    "#         start_stop_id (int): The starting stop ID.\n",
    "#         end_stop_id (int): The ending stop ID.\n",
    "#         stop_id_to_include (int): The stop ID for a transfer.\n",
    "#         max_transfers (int): The maximum number of transfers allowed.\n",
    "\n",
    "#     Returns:\n",
    "#         list: A list of unique paths (list of tuples) that satisfy the criteria, where each tuple contains:\n",
    "#               - route_id (int): The ID of the route.\n",
    "#               - stop_id (int): The ID of the stop.\n",
    "#     \"\"\"\n",
    "#     # Queue for BFS, with each element: (current stop, path taken, current transfers)\n",
    "#     queue = deque([(start_stop_id, [], 0)])  # (current stop, path, transfers)\n",
    "#     visited = set()\n",
    "#     valid_paths = []  # Store all paths meeting the criteria\n",
    "\n",
    "#     # Precompute a reverse map of stops to routes for efficient lookup\n",
    "#     stop_to_routes = defaultdict(list)\n",
    "#     for route_id, stops in route_to_stops.items():\n",
    "#         for stop in stops:\n",
    "#             stop_to_routes[stop].append(route_id)\n",
    "    \n",
    "#     while queue and len(valid_paths) < 5:  # Limit to 5 paths\n",
    "#         current_stop, path, transfers = queue.popleft()\n",
    "\n",
    "#         # Skip if we've already visited this stop with the same or fewer transfers\n",
    "#         if (current_stop, transfers) in visited:\n",
    "#             continue\n",
    "#         visited.add((current_stop, transfers))\n",
    "\n",
    "#         # Check if destination is reached within transfer limits\n",
    "#         if current_stop == end_stop_id and transfers <= max_transfers:\n",
    "#             if stop_id_to_include is None or any(stop == stop_id_to_include for _, stop in path):\n",
    "#                 valid_paths.append(path + [(None, current_stop)])\n",
    "#                 continue\n",
    "\n",
    "#         # Expand routes from the current stop using precomputed map\n",
    "#         for route_id in stop_to_routes[current_stop]:\n",
    "#             stops_in_route = route_to_stops[route_id]\n",
    "#             start_index = stops_in_route.index(current_stop)\n",
    "\n",
    "#             # Traverse only forward from the current stop to avoid revisits\n",
    "#             for next_stop in stops_in_route[start_index + 1:]:\n",
    "#                 if (next_stop, transfers + 1) not in visited:\n",
    "#                     # Add new stop to the queue with updated path and transfers\n",
    "#                     queue.append((next_stop, path + [(route_id, current_stop)], transfers + 1))\n",
    "\n",
    "#     print(f\"Found {len(valid_paths)} valid paths in total.\")\n",
    "#     return valid_paths[:5]  # Return up to 5 valid paths\n",
    "\n",
    "def pddl_planning(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    \"\"\"\n",
    "    Implement PDDL-style planning to find routes with optional transfers.\n",
    "\n",
    "    Args:\n",
    "        start_stop_id (int): The starting stop ID.\n",
    "        end_stop_id (int): The ending stop ID.\n",
    "        stop_id_to_include (int): The stop ID for a transfer.\n",
    "        max_transfers (int): The maximum number of transfers allowed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique paths (list of tuples) that satisfy the criteria, where each tuple contains:\n",
    "              - route_id (int): The ID of the route.\n",
    "              - stop_id (int): The ID of the stop.\n",
    "    \"\"\"\n",
    "    \n",
    "    # List to store the optimal routes\n",
    "    optimal_routes = []\n",
    "\n",
    "    # Step 1: Query all direct routes from start_stop_id to end_stop_id (both inclusive)\n",
    "    direct_routes = query_direct_routes(start_stop_id, end_stop_id)\n",
    "    \n",
    "    # If direct routes exist, add them to optimal routes\n",
    "    if direct_routes:\n",
    "        for route in direct_routes:\n",
    "            optimal_routes.append([(route, start_stop_id), (route, end_stop_id)])\n",
    "    \n",
    "    # Step 2: If no direct route or multiple transfers are allowed, we explore possible transfers\n",
    "    if not direct_routes or max_transfers > 0:\n",
    "        # Perform forward chaining to explore routes with potential transfer at stop_id_to_include\n",
    "        forward_routes = forward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers)\n",
    "        \n",
    "        # Add valid forward chaining routes\n",
    "        for route_path in forward_routes:\n",
    "            optimal_routes.append(route_path)\n",
    "        \n",
    "        # # Perform backward chaining to explore routes with potential transfer at stop_id_to_include\n",
    "        # backward_routes = backward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers)\n",
    "        \n",
    "        # # Add valid backward chaining routes\n",
    "        # for route_path in backward_routes:\n",
    "        #     optimal_routes.append(route_path)\n",
    "    \n",
    "    # Step 3: Return the list of optimal routes found\n",
    "    return optimal_routes\n",
    "\n",
    "# Function to filter fare data based on an initial fare limit\n",
    "def prune_data(merged_fare_df, initial_fare):\n",
    "    \"\"\"\n",
    "    Filter fare data based on an initial fare limit.\n",
    "\n",
    "    Args:\n",
    "        merged_fare_df (DataFrame): The merged fare DataFrame.\n",
    "        initial_fare (float): The maximum fare allowed.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A filtered DataFrame containing only routes within the fare limit.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame based on the initial fare constraint using 'price' instead of 'fare'\n",
    "    pruned_df = merged_fare_df[merged_fare_df['price'] <= initial_fare]\n",
    "    # print(f\"Pruned data to include fares within {initial_fare}. Remaining rows: {len(pruned_df)}\")\n",
    "    return pruned_df\n",
    "\n",
    "\n",
    "# Pre-computation of Route Summary\n",
    "def compute_route_summary(pruned_df):\n",
    "    \"\"\"\n",
    "    Generate a summary of routes based on fare information.\n",
    "\n",
    "    Args:\n",
    "        pruned_df (DataFrame): The filtered DataFrame containing fare information.\n",
    "\n",
    "    Returns:\n",
    "        dict: A summary of routes with the following structure:\n",
    "              {\n",
    "                  route_id (int): {\n",
    "                      'min_price': float,          # The minimum fare for the route\n",
    "                      'stops': set                # A set of stop IDs for that route\n",
    "                  }\n",
    "              }\n",
    "    \"\"\"\n",
    "    route_summary = {}\n",
    "\n",
    "    # Group by route_id to calculate min_price and collect stops for each route\n",
    "    for route_id, group in pruned_df.groupby('route_id'):\n",
    "        min_price = group['price'].min()  # Use 'price' instead of 'fare'\n",
    "        stops = set(group['origin_id']).union(group['destination_id'])  # Collect stops from origin and destination\n",
    "\n",
    "        route_summary[route_id] = {\n",
    "            'min_price': min_price,\n",
    "            'stops': stops\n",
    "        }\n",
    "    \n",
    "    # print(f\"Computed route summary for {len(route_summary)} routes.\")\n",
    "    return route_summary\n",
    "\n",
    "# BFS for optimized route planning with fare constraints\n",
    "\n",
    "\n",
    "def bfs_route_planner_optimized(start_stop_id, end_stop_id, initial_fare, route_summary, max_transfers=3):\n",
    "    \"\"\"\n",
    "    Use Breadth-First Search (BFS) to find the optimal route while considering fare constraints.\n",
    "\n",
    "    Args:\n",
    "        start_stop_id (int): The starting stop ID.\n",
    "        end_stop_id (int): The ending stop ID.\n",
    "        initial_fare (float): The available fare for the trip.\n",
    "        route_summary (dict): A summary of routes with fare and stop information.\n",
    "        max_transfers (int): The maximum number of transfers allowed (default is 3).\n",
    "\n",
    "    Returns:\n",
    "        list: A list representing the optimal route with stops and routes taken, structured as:\n",
    "              [\n",
    "                  (route_id (int), stop_id (int)),  # Tuple for each stop taken in the route\n",
    "                  ...\n",
    "              ]\n",
    "    \"\"\"\n",
    "    # Initialize queue for BFS: each entry is (current stop, route taken, path, remaining fare, transfers)\n",
    "    queue = deque([(start_stop_id, None, [], initial_fare, 0)])\n",
    "    visited = set()  # Keep track of (stop, route, transfers) to avoid revisiting the same route configuration\n",
    "\n",
    "    while queue:\n",
    "        current_stop, current_route, path, remaining_fare, transfers = queue.popleft()\n",
    "        \n",
    "        # If we reach the destination with remaining fare and within transfer limit, return the path\n",
    "        if current_stop == end_stop_id and transfers <= max_transfers:\n",
    "            # print(f\"Optimal path found with remaining fare {remaining_fare}: {path + [(current_route, current_stop)]}\")\n",
    "            # print(len(path))\n",
    "            if(len(path)==1):\n",
    "                # print([(path[0][0],current_stop)])\n",
    "                return [(path[0][0],current_stop)]\n",
    "            # return path + [(current_route, current_stop)]\n",
    "            # print([(path[0][0],path [1][1])] + [(current_route, current_stop)])\n",
    "            return [(path[0][0],path [1][1])] + [(current_route, current_stop)]\n",
    "\n",
    "        # Mark the current configuration as visited\n",
    "        visited.add((current_stop, current_route, transfers))\n",
    "\n",
    "        # Explore all routes available from the current stop\n",
    "        for route_id, route_data in route_summary.items():\n",
    "            if remaining_fare < route_data['min_price']:  # Skip routes exceeding remaining fare\n",
    "                continue\n",
    "            if current_stop not in route_data['stops']:  # Skip routes not containing the current stop\n",
    "                continue\n",
    "\n",
    "            # Calculate fare after taking the route and count transfers if switching routes\n",
    "            fare_after_route = remaining_fare - route_data['min_price']\n",
    "            new_transfers = transfers + (1 if route_id != current_route else 0)\n",
    "            \n",
    "            if new_transfers > max_transfers:\n",
    "                continue\n",
    "\n",
    "            # Enqueue all subsequent stops in this route\n",
    "            for stop_id in route_data['stops']:\n",
    "                if (stop_id, route_id, new_transfers) not in visited:\n",
    "                    queue.append((stop_id, route_id, path + [(route_id, current_stop)], fare_after_route, new_transfers))\n",
    "\n",
    "    print(\"No valid path found within fare and transfer constraints.\")\n",
    "    return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knowledge base loaded from file.\n",
      "Knowledge base loaded from file.\n"
     ]
    }
   ],
   "source": [
    "create_kb()  # Ensure the data is loaded before testing\n",
    "merged_fare_df = get_merged_fare_df()  # Use the function to retrieve the DataFrame\n",
    "initialize_datalog()\n",
    "# visualize_stop_route_graph_interactive(route_to_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in c:\\users\\himanshu\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\himanshu\\appdata\\roaming\\python\\python311\\site-packages (from memory_profiler) (5.9.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install memory_profiler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "def measure_performance(func, *args):\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = func(*args)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Execution Time: {end_time - start_time} seconds\")\n",
    "    print(f\"Peak Memory Usage: {peak / (1024 * 1024)} MB\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "\n",
    "def measure_brute_force(route_to_stops, start_stop, end_stop):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((direct_route_brute_force, (start_stop, end_stop)), interval=0.1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return execution_time, max(mem_usage) - min(mem_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def measure_fol_based(start_stop, end_stop):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((query_direct_routes, (start_stop, end_stop)), interval=0.1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return execution_time, max(mem_usage) - min(mem_usage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function to compare both implementations\n",
    "def analyze_route_search(route_to_stops, start_stop, end_stop):\n",
    "    # Brute-force analysis\n",
    "    bf_time, bf_memory = measure_brute_force(route_to_stops, start_stop, end_stop)\n",
    "    \n",
    "    # FOL-based analysis\n",
    "    fol_time, fol_memory = measure_fol_based(start_stop, end_stop)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Brute-Force Approach:\")\n",
    "    print(f\"Execution Time: {bf_time:.4f} seconds\")\n",
    "    print(f\"Memory Usage: {bf_memory:.4f} MB\")\n",
    "    \n",
    "    print(\"\\nFOL-Based Approach:\")\n",
    "    print(f\"Execution Time: {fol_time:.4f} seconds\")\n",
    "    print(f\"Memory Usage: {fol_memory:.4f} MB\")\n",
    "    \n",
    "    # Comparison\n",
    "    print(\"\\nComparison of Steps:\")\n",
    "    print(f\"- Brute-Force: Iterates over routes and stops.\")\n",
    "    print(f\"- FOL-Based: Uses declarative inference with predicates.\")\n",
    "    print(\"FOL-based approach abstracts away iteration and relies on knowledge base inference.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparison of Steps:\n",
      "- Brute-Force: Iterates over routes and stops.\n",
      "- FOL-Based: Uses declarative inference with predicates.\n",
      "FOL-based approach abstracts away iteration and relies on knowledge base inference.\n"
     ]
    }
   ],
   "source": [
    "    print(\"\\nComparison of Steps:\")\n",
    "    print(f\"- Brute-Force: Iterates over routes and stops.\")\n",
    "    print(f\"- FOL-Based: Uses declarative inference with predicates.\")\n",
    "    print(\"FOL-based approach abstracts away iteration and relies on knowledge base inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure performance for test case : 2001, 2005 \n",
      "Brute force Performance:\n",
      "Execution Time: 0.001001119613647461 seconds\n",
      "Peak Memory Usage: 0.0001068115234375 MB\n",
      "\n",
      "query optimisation Performance:\n",
      "Execution Time: 0.003998756408691406 seconds\n",
      "Peak Memory Usage: 0.0219268798828125 MB\n",
      "measure performance for test case :2573, 1177 \n",
      "Brute force Performance:\n",
      "Execution Time: 0.0010013580322265625 seconds\n",
      "Peak Memory Usage: 0.0001068115234375 MB\n",
      "\n",
      "query optimisation Performance:\n",
      "Execution Time: 0.003306150436401367 seconds\n",
      "Peak Memory Usage: 0.02225494384765625 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1117, 1407, 10001]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# analyze_route_search(route_to_stops, 2573, 1177)\n",
    "# # 2001, 2005\n",
    "# analyze_route_search(route_to_stops, 2001, 2005)\n",
    "# Run forward chaining and measure performance\n",
    "print(\"measure performance for test case : 2001, 2005 \")\n",
    "print(\"Brute force Performance:\")\n",
    "measure_performance(direct_route_brute_force, 2001, 2005)\n",
    "\n",
    "# Run backward chaining and measure performance\n",
    "print(\"\\nquery optimisation Performance:\")\n",
    "measure_performance(query_direct_routes,  2001, 2005)\n",
    "\n",
    "print(\"measure performance for test case :2573, 1177 \")\n",
    "print(\"Brute force Performance:\")\n",
    "measure_performance(direct_route_brute_force, 2573, 1177)\n",
    "\n",
    "# Run backward chaining and measure performance\n",
    "print(\"\\nquery optimisation Performance:\")\n",
    "measure_performance(query_direct_routes,  2573, 1177)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward_chaining:\n",
      "Execution Time: 1.7526 seconds\n",
      "Memory Usage: 0.0039 MB\n",
      "\n",
      "backward chaining:\n",
      "Execution Time: 1.8001 seconds\n",
      "Memory Usage: 0.0000 MB\n",
      "forward_chaining:\n",
      "Execution Time: 2.0317 seconds\n",
      "Memory Usage: 0.0039 MB\n",
      "\n",
      "backward chaining:\n",
      "Execution Time: 1.8099 seconds\n",
      "Memory Usage: 0.0000 MB\n"
     ]
    }
   ],
   "source": [
    "from memory_profiler import memory_usage\n",
    "import time\n",
    "\n",
    "def measure_forward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((forward_chaining, (start_stop_id, end_stop_id, stop_id_to_include, max_transfers)), interval=0.1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return execution_time, max(mem_usage) - min(mem_usage)\n",
    "def measure_backward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((backward_chaining, (start_stop_id, end_stop_id, stop_id_to_include, max_transfers)), interval=0.1)\n",
    "    execution_time = time.time() - start_time\n",
    "    return execution_time, max(mem_usage) - min(mem_usage)\n",
    "\n",
    "# Example function to compare both implementations\n",
    "def analyze_route_search(start_stop_id, end_stop_id, stop_id_to_include, max_transfers):\n",
    "    # Brute-force analysis\n",
    "    fc_time, fc_memory = measure_forward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers)\n",
    "    \n",
    "    # FOL-based analysis\n",
    "    fol_time, fol_memory = measure_backward_chaining(start_stop_id, end_stop_id, stop_id_to_include, max_transfers)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"forward_chaining:\")\n",
    "    print(f\"Execution Time: {fc_time:.4f} seconds\")\n",
    "    print(f\"Memory Usage: {fc_memory:.4f} MB\")\n",
    "    \n",
    "    print(\"\\nbackward chaining:\")\n",
    "    print(f\"Execution Time: {fol_time:.4f} seconds\")\n",
    "    print(f\"Memory Usage: {fol_memory:.4f} MB\")\n",
    "    \n",
    "    # Comparison\n",
    "    # print(\"\\nComparison of Steps:\")\n",
    "    # print(f\"- Brute-Force: Iterates over routes and stops.\")\n",
    "    # print(f\"- FOL-Based: Uses declarative inference with predicates.\")\n",
    "    # print(\"FOL-based approach abstracts away iteration and relies on knowledge base inference.\")\n",
    "analyze_route_search(22540, 2573, 4686, 1)\n",
    "analyze_route_search(951, 340, 300, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tracemalloc\n",
    "\n",
    "def measure_performance(func, *args):\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    result = func(*args)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    current, peak = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "\n",
    "    print(f\"Execution Time: {end_time - start_time} seconds\")\n",
    "    print(f\"Peak Memory Usage: {peak / (1024 * 1024)} MB\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "measure performance for :(951, 340, 300, 1)\n",
      "Forward Chaining Performance:\n",
      "Execution Time: 0.07586860656738281 seconds\n",
      "Peak Memory Usage: 0.09922409057617188 MB\n",
      "\n",
      "Backward Chaining Performance:\n",
      "Execution Time: 0.008977174758911133 seconds\n",
      "Peak Memory Usage: 0.06063652038574219 MB\n",
      "measure performance for :(22540, 2573, 4686, 1)\n",
      "Forward Chaining Performance:\n",
      "Execution Time: 0.004985332489013672 seconds\n",
      "Peak Memory Usage: 0.022439002990722656 MB\n",
      "\n",
      "Backward Chaining Performance:\n",
      "Execution Time: 0.002103090286254883 seconds\n",
      "Peak Memory Usage: 0.027588844299316406 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1407, 4686, 10153)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample function calls with dummy data\n",
    "\n",
    "# # Define test arguments\n",
    "# start_stop_id = 1\n",
    "# end_stop_id = 10\n",
    "# stop_id_to_include = 5\n",
    "# max_transfers = 1\n",
    "print(\"measure performance for :(951, 340, 300, 1)\")\n",
    "# Run forward chaining and measure performance\n",
    "print(\"Forward Chaining Performance:\")\n",
    "measure_performance(forward_chaining, 951, 340, 300, 1)\n",
    "\n",
    "# Run backward chaining and measure performance\n",
    "print(\"\\nBackward Chaining Performance:\")\n",
    "measure_performance(backward_chaining, 951, 340, 300, 1)\n",
    "print(\"measure performance for :(22540, 2573, 4686, 1)\")\n",
    "print(\"Forward Chaining Performance:\")\n",
    "measure_performance(forward_chaining, 22540, 2573, 4686, 1)\n",
    "\n",
    "# Run backward chaining and measure performance\n",
    "print(\"\\nBackward Chaining Performance:\")\n",
    "measure_performance(backward_chaining, 22540, 2573, 4686, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After executing both methods with step counting and logging, you could summarize the findings like this:\n",
    "\n",
    "Execution Path:\n",
    "\n",
    "In Forward Chaining, we iterate over each route sequentially, which results in more steps as it evaluates the possible via stops and interchanges at each level.\n",
    "Backward Chaining may take fewer steps because it can eliminate paths earlier if they donâ€™t reach the required stops, especially when there are fewer interchanges.\n",
    "Overall Comparison:\n",
    "\n",
    "Forward Chaining typically involves more steps due to the depth-first exploration of each potential path.\n",
    "Backward Chaining can be more optimal in certain cases due to its ability to work backward from the goal and reject paths sooner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Chaining\n",
    "Process: In Forward Chaining, the search starts from the starting stop and explores all possible routes forward to reach the destination. This approach evaluates each route from the beginning and continues until it finds a valid path that meets the criteria (such as including a via stop and a maximum of one interchange).\n",
    "Steps: This can lead to more overall steps because it needs to evaluate each forward path sequentially from the start, potentially evaluating many intermediate stops before finding a valid route that meets all conditions.\n",
    "Best Case: If a valid via stop is close to the start stop, Forward Chaining can potentially find a solution more quickly by exploring forward.\n",
    "Worst Case: Forward Chaining can take longer if valid paths are deep in the search space, as it may need to evaluate all possible paths forward, even those that donâ€™t meet the criteria.\n",
    "Backward Chaining\n",
    "Process: In Backward Chaining, the search begins at the destination and works backward to find possible routes leading to the starting stop, checking if a route passes through the via stop and meets the interchange constraint.\n",
    "Steps: This can result in fewer steps overall because paths can be eliminated earlier if they do not lead back to the starting stop or meet the via stop requirement. Backward Chaining often stops searching sooner because invalid paths are discarded early.\n",
    "Best Case: If valid routes are closer to the end stop, Backward Chaining will reach them quickly and avoid unnecessary paths.\n",
    "Worst Case: In cases with many possible routes leading to dead ends or failing constraints, Backward Chaining might still need to explore a large number of paths but will generally perform fewer steps than Forward Chaining.\n",
    "Overall Comparison\n",
    "In most cases, Backward Chaining will tend to take fewer steps than Forward Chaining, particularly when:\n",
    "\n",
    "There are many routes, but only a few meet the via stop and interchange constraints.\n",
    "The solution space contains many paths that can be quickly ruled out from the destination side.\n",
    "Conclusion: Given the constraint of including a via stop and limiting interchanges, Backward Chaining is typically more efficient in terms of step count, as it can filter out invalid paths sooner than Forward Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pddl- planning :\n",
      "Execution Time: 0.09207820892333984 seconds\n",
      "Peak Memory Usage: 0.09199047088623047 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(37, 300, 712),\n",
       " (49, 300, 712),\n",
       " (121, 300, 712),\n",
       " (387, 300, 712),\n",
       " (1038, 300, 712),\n",
       " (1211, 300, 712),\n",
       " (1571, 300, 712),\n",
       " (10433, 300, 712),\n",
       " (10453, 300, 712)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"pddl- planning :\")\n",
    "measure_performance(pddl_planning, 951, 340, 300, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall Comparison of Steps Involved in Each Approach:\n",
    "Forward Chaining:\n",
    "\n",
    "Number of steps: High due to exhaustive search and constraints applied only after exploring possible routes.\n",
    "Reason: It explores all paths forward, even if they might violate constraints, leading to more steps before a valid solution is found.\n",
    "Efficiency: Less efficient in terms of steps due to potential recomputation of many paths.\n",
    "Backward Chaining:\n",
    "\n",
    "Number of steps: Lower compared to Forward Chaining because it narrows the search space by focusing on paths that can directly lead to the goal.\n",
    "Reason: It starts from the goal and works backward, pruning paths that do not lead directly to the goal early in the process.\n",
    "Efficiency: More efficient in terms of the number of steps because it quickly discards invalid paths and focuses on those leading to the goal.\n",
    "PDDL:\n",
    "\n",
    "Number of steps: Moderate to Low depending on the planner and its ability to handle constraints efficiently.\n",
    "Reason: PDDL planners apply constraints and actions in a structured manner, reducing the search space by enforcing the constraints at the beginning of the search. However, it might require more steps than Backward Chaining if the planner doesn't prune paths efficiently.\n",
    "Efficiency: The efficiency of PDDL depends on the planner used. Some planners may require fewer steps by avoiding unnecessary explorations, while others might require more depending on the complexity of the search space.\n",
    "Conclusion:\n",
    "Forward Chaining typically involves the most steps, as it exhaustively explores all possibilities before considering constraints, leading to higher computational cost.\n",
    "Backward Chaining usually involves fewer steps, as it focuses on paths that lead directly to the goal, pruning invalid paths early.\n",
    "PDDL generally involves moderate steps, but its efficiency depends heavily on the specific planner and the way it handles the search space and constraints.\n",
    "Thus, Backward Chaining tends to be the most efficient in terms of the number of steps involved, followed by PDDL, with Forward Chaining usually requiring the highest number of steps.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The three approaches (Forward Chaining, Backward Chaining, and Planning Domain Definition Language - PDDL) will not always produce the same optimal route for the following reasons:\n",
    "\n",
    "1. Forward Chaining\n",
    "How it works: Forward Chaining starts from the initial state (start stop) and works its way forward to the goal state (destination stop). It explores all possible paths from the start, considering the available actions (boarding routes, transferring between routes) at each stop.\n",
    "Constraints application: Forward Chaining will encounter all possible intermediate steps, but because it explores each path forward, it can encounter and apply the constraints (such as including a via stop and limiting interchanges) later in the process. If a path violates the constraints, it may backtrack and try another route. Forward Chaining's search might involve revisiting or recalculating multiple paths, leading to more computational effort and potentially suboptimal routes if the path exploration is too extensive.\n",
    "Risk of suboptimal routes: In some cases, if Forward Chaining does not prioritize paths that immediately meet constraints (e.g., via stop or interchange limits), it might explore more paths than necessary, which could lead to suboptimal solutions, especially in a large search space.\n",
    "2. Backward Chaining\n",
    "How it works: Backward Chaining starts from the goal (end stop) and works backwards, trying to find paths that lead back to the starting stop, considering the transfer points and route connections.\n",
    "Constraints application: Backward Chaining can more efficiently discard invalid paths because it works backwards and quickly eliminates paths that don't lead to the goal. By checking the conditions from the end, it can sometimes avoid exploring paths that are already invalid, which makes it faster and more efficient in some cases.\n",
    "Risk of suboptimal routes: Since Backward Chaining only explores routes that are viable from the destination backwards, it might ignore some potential solutions that Forward Chaining might catch. This could lead to missing an optimal path, especially in cases where a path from the start to the destination has a more efficient way forward but isn't included due to a narrow backward search scope.\n",
    "3. Planning Domain Definition Language (PDDL)\n",
    "How it works: PDDL allows you to define a formalized problem in terms of actions, states, and goals. For route planning, you define actions such as \"board route\" and \"transfer between routes,\" the initial state (starting stop), and the goal state (destination stop). PDDL uses search algorithms like STRIPS to find the best sequence of actions to achieve the goal state.\n",
    "Constraints application: PDDL allows you to formally encode constraints like having an intermediate stop (via stop) and limiting interchanges. Depending on the search strategy and the PDDL planner used, the constraints will be enforced at each step of the planning process, potentially pruning non-optimal paths. PDDL planners tend to apply constraints upfront during the planning process, helping to avoid exploring invalid routes early.\n",
    "Risk of suboptimal routes: The efficiency of the PDDL planner depends on the complexity of the planner and how it handles the constraints. Some PDDL planners might focus on finding the quickest solution, which can sometimes lead to ignoring paths that meet the constraints but are slightly longer in terms of the number of steps or interchanges.\n",
    "Comparison and Potential Suboptimality\n",
    "Forward Chaining: Can lead to suboptimal solutions if it doesn't prioritize the constraint application early enough. It might explore many paths before applying the constraints, resulting in inefficiency.\n",
    "\n",
    "Backward Chaining: Tends to perform better in terms of eliminating paths earlier. However, it could still miss optimal solutions if a viable path from the start to the goal is not explored in the backward search.\n",
    "\n",
    "PDDL: Tends to be the most structured approach, as constraints and goals are formally defined. However, depending on the planner's search algorithm, it might either over-prioritize efficiency (leading to suboptimal routes) or under-prioritize constraint satisfaction, depending on the planner's configuration and approach.\n",
    "\n",
    "Conclusion\n",
    "Do all algorithms produce the same optimal route? No, because each algorithm has different ways of exploring the search space and applying constraints.\n",
    "Forward Chaining might explore more paths than necessary and could produce suboptimal routes.\n",
    "Backward Chaining might discard paths earlier but may also miss solutions that Forward Chaining could find.\n",
    "PDDL provides a formalized approach but can be limited by the specific planner's performance in enforcing constraints and finding the optimal path.\n",
    "The key takeaway is that the application of constraints and the search strategy (whether forward or backward) can lead to different exploration patterns, meaning some algorithms might explore suboptimal paths or fail to find the optimal route based on how constraints are applied during the search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
